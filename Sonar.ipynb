{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37504802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e66a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "\n",
    "    def __init__(self, mean=None, std=None, epsilon=1e-7):\n",
    "        \"\"\"Standard Scaler.\n",
    "        The class can be used to normalize PyTorch Tensors using native functions. The module does not expect the\n",
    "        tensors to be of any specific shape; as long as the features are the last dimension in the tensor, the module\n",
    "        will work fine.\n",
    "        :param mean: The mean of the features. The property will be set after a call to fit.\n",
    "        :param std: The standard deviation of the features. The property will be set after a call to fit.\n",
    "        :param epsilon: Used to avoid a Division-By-Zero exception.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, values):\n",
    "        dims = list(range(values.dim() - 1))\n",
    "        self.mean = torch.mean(values, dim=dims)\n",
    "        self.std = torch.std(values, dim=dims)\n",
    "        \n",
    "\n",
    "    def transform(self, values):\n",
    "        return (values - self.mean) / (self.std + self.epsilon)\n",
    "\n",
    "    def fit_transform(self, values):\n",
    "        self.fit(values)\n",
    "        return self.transform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db9c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class DatasetSonar(Dataset):\n",
    "    def __init__(self, path, header = 'infer'):\n",
    "    \n",
    "        self.df = pd.read_csv(path, header = None)\n",
    "        self.data = self.df.values[:, :-1]\n",
    "        self.data = self.data.astype('float32')\n",
    "        \n",
    "        \n",
    "        self.labels = self.df.values[:, -1]\n",
    "        \n",
    "        display(self.labels)\n",
    "      \n",
    "        YConversion = pd.DataFrame()\n",
    "        YConversion[\"R\"] = (self.df[60]==\"R\").apply(lambda x : 1.0 if x else 0.0)\n",
    "        YConversion[\"M\"] = (self.df[60]==\"M\").apply(lambda x : 1.0 if x else 0.0)\n",
    "        \n",
    "        y_tensor = torch.as_tensor(YConversion.to_numpy()).type(torch.float32)\n",
    "        \n",
    "        display(y_tensor)\n",
    "        \n",
    "        self.labels = self.labels.reshape(-1, 1)\n",
    "        \n",
    "        # Tensores        \n",
    "        x_tensor = torch.tensor(self.data)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(x_tensor)\n",
    "        XScalada = scaler.transform(x_tensor).type(torch.float32)\n",
    "        self.data = torch.cat((XScalada,y_tensor),1)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx][0:60], self.data[idx][60:62])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self.df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0bdf825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
       "       'R', 'R', 'R', 'R', 'R', 'R', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
       "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-0.3986, -0.0406, -0.0269, -0.7134,  0.3636, -0.1010,  0.5204,  0.2971,\n",
       "          1.1226,  0.0211, -0.5660, -0.6570, -0.3512, -1.4110, -1.2374, -0.6498,\n",
       "         -0.4019, -0.5828,  0.0116, -0.3173, -0.1193, -0.4579, -0.8561, -0.4920,\n",
       "         -0.0177, -0.2460,  0.0336,  0.4805,  0.1541, -0.8844, -1.7467, -0.8378,\n",
       "          0.4594,  1.5199,  1.7795,  1.7638,  1.2729,  1.2680,  0.8464, -0.2060,\n",
       "         -1.3924,  0.0303,  0.2587,  1.5869,  0.4410, -0.1645, -0.1996,  0.6869,\n",
       "         -0.3791,  0.8764,  0.5938, -1.1127, -0.5962,  0.6792, -0.2949,  1.4780,\n",
       "          1.7595,  0.0697,  0.1713, -0.6573]),\n",
       " tensor([1., 0.]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cargamos los datos y comprobamos que tenemos salida\n",
    "dataset = DatasetSonar(\"data/sonar.csv\")\n",
    "display(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d8d9fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tam dataset: 208 train: 166 tamVal: 42\n"
     ]
    }
   ],
   "source": [
    "# División en train e test\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "lonxitudeDataset = len(dataset)\n",
    "tamTrain =int(lonxitudeDataset*0.8)\n",
    "tamVal = lonxitudeDataset - tamTrain\n",
    "print(f\"Tam dataset: {lonxitudeDataset} train: {tamTrain} tamVal: {tamVal}\")\n",
    "train_set, val_set = random_split(dataset,[tamTrain,tamVal])\n",
    "train_ldr = torch.utils.data.DataLoader(train_set, batch_size=2,\n",
    "    shuffle=True, drop_last=False)\n",
    "validation_loader =torch.utils.data.DataLoader(val_set, batch_size=4, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59a7d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(60, 50)\n",
    "        self.layer2 = nn.Linear(50, 50)\n",
    "        self.layer3 = nn.Linear(in_features=50, out_features=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.softmax(self.layer3(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "055ab43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer1): Linear(in_features=60, out_features=50, bias=True)\n",
       "  (layer2): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (layer3): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model     = Model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn   = nn.CrossEntropyLoss()\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eacecd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7248, -0.7050, -0.5733, -0.7822, -0.7021, -0.5917, -0.5575, -0.8596,\n",
      "         -0.4460, -0.5807, -1.1093, -1.4294, -1.4933, -1.0285, -1.1688, -1.3079,\n",
      "         -0.8009, -0.6015, -0.7191, -0.4049, -0.0510,  0.3354,  0.7016,  0.7103,\n",
      "          0.9435,  1.1817,  1.2124,  0.7638,  0.4838,  0.3654,  0.3464,  0.3459,\n",
      "          0.2924,  0.4159,  0.4906,  0.4629,  0.3914,  0.0509, -0.7098, -0.6840,\n",
      "         -0.5409, -1.0324, -1.3781, -1.2370, -0.8905, -0.8223, -1.1242, -1.1139,\n",
      "         -1.1606, -1.1507, -1.1799, -0.9674, -0.4687, -1.0877, -0.6053, -0.5965,\n",
      "         -1.0578, -0.5949,  0.1066, -0.1406],\n",
      "        [-0.8509,  0.0596,  0.6341,  0.3204,  0.2430, -0.1924, -0.4248,  0.9301,\n",
      "          0.6470,  0.6468,  1.2546,  1.8889,  1.5380,  0.7061,  0.1816, -0.1452,\n",
      "         -0.4141, -0.6034, -0.2702,  0.6219,  0.7976,  0.7270,  0.0780,  0.0968,\n",
      "          0.4057,  0.7121,  1.2124,  1.2651,  0.9874,  0.3047,  0.3884,  0.9462,\n",
      "          0.3873, -0.5524, -1.3992, -0.1865, -0.1963,  0.3458,  1.5185,  1.0438,\n",
      "         -0.1634,  1.1344,  1.6840,  2.0858,  2.1907,  0.4552,  0.2030,  1.5601,\n",
      "          1.1896, -0.5140,  1.2434,  1.0670, -0.4120,  0.4738,  1.0877, -1.2241,\n",
      "          0.5323, -0.6722, -0.5891, -0.9356]])\n"
     ]
    }
   ],
   "source": [
    "entradaProba,dest = next(iter(train_ldr))\n",
    "print(entradaProba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e92be0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4464, -0.2317, -0.6280,  0.1786, -1.2097, -0.4343,  0.3602,  0.1210,\n",
       "          0.0076,  0.3269,  0.1860,  0.8623,  0.8321, -0.1786, -0.2366,  0.1711,\n",
       "          1.0726,  0.4626, -1.0939, -1.1420,  0.5362,  0.3987, -1.2298, -1.6321,\n",
       "         -0.2863, -0.4644, -0.8363,  0.7065,  0.7984, -0.7816, -1.2976, -1.2406,\n",
       "         -1.7748, -1.0570, -0.3418, -0.9388, -0.8474, -0.6271, -1.2679, -1.6288,\n",
       "         -0.7332, -0.7112, -0.4572, -0.1911, -0.6360, -0.7043, -1.1978, -0.8175,\n",
       "         -1.1439, -0.4482, -0.5803, -0.9986, -0.1571,  0.4464,  1.3699,  1.0073,\n",
       "          0.3595,  0.1933, -0.4435,  0.1973],\n",
       "        [-0.3507, -0.7778, -1.1016, -0.1911, -0.0738,  0.8752,  0.4864, -0.1667,\n",
       "         -0.0963, -0.5487, -1.1869, -1.5244, -1.0684, -0.2138,  0.5248,  1.0046,\n",
       "          1.2421,  1.3791,  1.5341,  1.5125,  1.1913,  1.1323,  0.5521, -0.6660,\n",
       "         -1.3536, -1.5633, -1.8117, -1.3724, -0.2729,  0.1725, -0.2018, -0.3636,\n",
       "          0.1268, -0.0728, -0.3213, -0.6476, -1.0529, -0.9440, -0.8715, -1.1844,\n",
       "         -0.7297, -0.5221, -0.8291, -0.2736,  0.0183, -0.6281, -0.5722, -0.2647,\n",
       "         -0.1955, -0.5140,  0.1109, -0.1786, -1.1910, -0.8548, -0.6194, -0.8929,\n",
       "         -0.9368, -0.6258, -0.4597, -0.9952]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deseada:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5064, 0.4936],\n",
       "        [0.4739, 0.5261]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6771, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prueba\n",
    "entradaProba,dest = next(iter(train_ldr))\n",
    "print(\"Entrada:\")\n",
    "display(entradaProba)\n",
    "print(\"Deseada:\")\n",
    "display(dest)\n",
    "saida = model(entradaProba) \n",
    "print(\"Salida:\")\n",
    "display(saida)\n",
    "loss_fn(saida, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a23a75",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6f7c10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(train_ldr) instead of\n",
    "    # iter(train_ldr) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_ldr):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afb3720b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 0.31326217353343966\n",
      "  batch 20 loss: 0.3132625877857208\n",
      "  batch 30 loss: 0.313262340426445\n",
      "  batch 40 loss: 0.31326272189617155\n",
      "  batch 50 loss: 0.3132624298334122\n",
      "  batch 60 loss: 0.31326261460781096\n",
      "  batch 70 loss: 0.3132622689008713\n",
      "  batch 80 loss: 0.3132621109485626\n",
      "LOSS train 0.3132621109485626 valid 0.44672584533691406 16.0/41\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.3132626533508301\n",
      "  batch 20 loss: 0.3132625848054886\n",
      "  batch 30 loss: 0.31326223611831666\n",
      "  batch 40 loss: 0.31326216757297515\n",
      "  batch 50 loss: 0.313263002038002\n",
      "  batch 60 loss: 0.31326251924037934\n",
      "  batch 70 loss: 0.3132623463869095\n",
      "  batch 80 loss: 0.31326197683811186\n",
      "LOSS train 0.31326197683811186 valid 0.4466857612133026 16.0/41\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.31326254904270173\n",
      "  batch 20 loss: 0.31326245367527006\n",
      "  batch 30 loss: 0.31326213777065276\n",
      "  batch 40 loss: 0.313262465596199\n",
      "  batch 50 loss: 0.31326225101947786\n",
      "  batch 60 loss: 0.3132625788450241\n",
      "  batch 70 loss: 0.31326261460781096\n",
      "  batch 80 loss: 0.31326210498809814\n",
      "LOSS train 0.31326210498809814 valid 0.44672691822052 16.0/41\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.313262078166008\n",
      "  batch 20 loss: 0.3132623881101608\n",
      "  batch 30 loss: 0.31326259672641754\n",
      "  batch 40 loss: 0.31326221525669096\n",
      "  batch 50 loss: 0.3132624804973602\n",
      "  batch 60 loss: 0.3132619321346283\n",
      "  batch 70 loss: 0.3132630378007889\n",
      "  batch 80 loss: 0.3132621169090271\n",
      "LOSS train 0.3132621169090271 valid 0.44653844833374023 16.0/41\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.3132622063159943\n",
      "  batch 20 loss: 0.31326223313808443\n",
      "  batch 30 loss: 0.31326216757297515\n",
      "  batch 40 loss: 0.31326234340667725\n",
      "  batch 50 loss: 0.3132623374462128\n",
      "  batch 60 loss: 0.3132626563310623\n",
      "  batch 70 loss: 0.31326215267181395\n",
      "  batch 80 loss: 0.3132622718811035\n",
      "LOSS train 0.3132622718811035 valid 0.44665318727493286 16.0/41\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.31326265037059786\n",
      "  batch 20 loss: 0.31326223611831666\n",
      "  batch 30 loss: 0.31326210498809814\n",
      "  batch 40 loss: 0.31326269805431367\n",
      "  batch 50 loss: 0.31326222121715547\n",
      "  batch 60 loss: 0.31326186656951904\n",
      "  batch 70 loss: 0.3132623046636581\n",
      "  batch 80 loss: 0.3132624000310898\n",
      "LOSS train 0.3132624000310898 valid 0.4467441737651825 16.0/41\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.3132619023323059\n",
      "  batch 20 loss: 0.31326212286949157\n",
      "  batch 30 loss: 0.31326213479042053\n",
      "  batch 40 loss: 0.31326272189617155\n",
      "  batch 50 loss: 0.3132623344659805\n",
      "  batch 60 loss: 0.31326206028461456\n",
      "  batch 70 loss: 0.31326272189617155\n",
      "  batch 80 loss: 0.31326221227645873\n",
      "LOSS train 0.31326221227645873 valid 0.4467497766017914 16.0/41\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.31326215863227846\n",
      "  batch 20 loss: 0.3132618486881256\n",
      "  batch 30 loss: 0.313262414932251\n",
      "  batch 40 loss: 0.3132625073194504\n",
      "  batch 50 loss: 0.3132620632648468\n",
      "  batch 60 loss: 0.3132624626159668\n",
      "  batch 70 loss: 0.31326199173927305\n",
      "  batch 80 loss: 0.313262078166008\n",
      "LOSS train 0.313262078166008 valid 0.4469427168369293 16.0/41\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.3132622420787811\n",
      "  batch 20 loss: 0.31326200664043424\n",
      "  batch 30 loss: 0.31326174139976504\n",
      "  batch 40 loss: 0.31326268017292025\n",
      "  batch 50 loss: 0.31326223313808443\n",
      "  batch 60 loss: 0.313262015581131\n",
      "  batch 70 loss: 0.3132623374462128\n",
      "  batch 80 loss: 0.3132624924182892\n",
      "LOSS train 0.3132624924182892 valid 0.44658997654914856 16.0/41\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.31326223015785215\n",
      "  batch 20 loss: 0.3132622271776199\n",
      "  batch 30 loss: 0.31326210498809814\n",
      "  batch 40 loss: 0.3132623791694641\n",
      "  batch 50 loss: 0.3132622420787811\n",
      "  batch 60 loss: 0.3132620394229889\n",
      "  batch 70 loss: 0.31326204538345337\n",
      "  batch 80 loss: 0.31326221525669096\n",
      "LOSS train 0.31326221525669096 valid 0.4466257393360138 18.0/41\n",
      "EPOCH 11:\n",
      "  batch 10 loss: 0.3132621169090271\n",
      "  batch 20 loss: 0.3132618248462677\n",
      "  batch 30 loss: 0.3132620096206665\n",
      "  batch 40 loss: 0.3132621169090271\n",
      "  batch 50 loss: 0.3132624089717865\n",
      "  batch 60 loss: 0.3132623642683029\n",
      "  batch 70 loss: 0.31326249837875364\n",
      "  batch 80 loss: 0.3132620006799698\n",
      "LOSS train 0.3132620006799698 valid 0.4464034140110016 18.0/41\n",
      "EPOCH 12:\n",
      "  batch 10 loss: 0.3132623642683029\n",
      "  batch 20 loss: 0.3132622092962265\n",
      "  batch 30 loss: 0.3132621794939041\n",
      "  batch 40 loss: 0.31326206028461456\n",
      "  batch 50 loss: 0.3132620841264725\n",
      "  batch 60 loss: 0.3132620811462402\n",
      "  batch 70 loss: 0.3132619738578796\n",
      "  batch 80 loss: 0.3132620811462402\n",
      "LOSS train 0.3132620811462402 valid 0.446681946516037 18.0/41\n",
      "EPOCH 13:\n",
      "  batch 10 loss: 0.31326218247413634\n",
      "  batch 20 loss: 0.31326215863227846\n",
      "  batch 30 loss: 0.313261952996254\n",
      "  batch 40 loss: 0.31326216757297515\n",
      "  batch 50 loss: 0.31326241195201876\n",
      "  batch 60 loss: 0.3132621645927429\n",
      "  batch 70 loss: 0.31326188445091246\n",
      "  batch 80 loss: 0.31326201260089875\n",
      "LOSS train 0.31326201260089875 valid 0.446522980928421 18.0/41\n",
      "EPOCH 14:\n",
      "  batch 10 loss: 0.3132621616125107\n",
      "  batch 20 loss: 0.3132619857788086\n",
      "  batch 30 loss: 0.3132622390985489\n",
      "  batch 40 loss: 0.31326220333576205\n",
      "  batch 50 loss: 0.31326195895671843\n",
      "  batch 60 loss: 0.3132617145776749\n",
      "  batch 70 loss: 0.3132621020078659\n",
      "  batch 80 loss: 0.313262340426445\n",
      "LOSS train 0.313262340426445 valid 0.44657090306282043 18.0/41\n",
      "EPOCH 15:\n",
      "  batch 10 loss: 0.31326202750205995\n",
      "  batch 20 loss: 0.3132621258497238\n",
      "  batch 30 loss: 0.3132619887590408\n",
      "  batch 40 loss: 0.3132618278264999\n",
      "  batch 50 loss: 0.31326242685317995\n",
      "  batch 60 loss: 0.31326199769973756\n",
      "  batch 70 loss: 0.3132618755102158\n",
      "  batch 80 loss: 0.3132622480392456\n",
      "LOSS train 0.3132622480392456 valid 0.446577250957489 18.0/41\n",
      "EPOCH 16:\n",
      "  batch 10 loss: 0.31326180398464204\n",
      "  batch 20 loss: 0.31326210498809814\n",
      "  batch 30 loss: 0.31326231062412263\n",
      "  batch 40 loss: 0.3132621258497238\n",
      "  batch 50 loss: 0.31326197981834414\n",
      "  batch 60 loss: 0.31326195895671843\n",
      "  batch 70 loss: 0.31326192915439605\n",
      "  batch 80 loss: 0.31326196193695066\n",
      "LOSS train 0.31326196193695066 valid 0.44673439860343933 18.0/41\n",
      "EPOCH 17:\n",
      "  batch 10 loss: 0.31326188147068024\n",
      "  batch 20 loss: 0.31326196789741517\n",
      "  batch 30 loss: 0.3132619857788086\n",
      "  batch 40 loss: 0.3132620692253113\n",
      "  batch 50 loss: 0.3132619112730026\n",
      "  batch 60 loss: 0.31326226592063905\n",
      "  batch 70 loss: 0.3132618486881256\n",
      "  batch 80 loss: 0.31326235830783844\n",
      "LOSS train 0.31326235830783844 valid 0.44634029269218445 18.0/41\n",
      "EPOCH 18:\n",
      "  batch 10 loss: 0.3132620513439178\n",
      "  batch 20 loss: 0.31326191425323485\n",
      "  batch 30 loss: 0.3132621765136719\n",
      "  batch 40 loss: 0.31326198279857637\n",
      "  batch 50 loss: 0.3132618755102158\n",
      "  batch 60 loss: 0.3132620811462402\n",
      "  batch 70 loss: 0.3132620006799698\n",
      "  batch 80 loss: 0.31326186656951904\n",
      "LOSS train 0.31326186656951904 valid 0.4467870593070984 18.0/41\n",
      "EPOCH 19:\n",
      "  batch 10 loss: 0.31326189935207366\n",
      "  batch 20 loss: 0.3132620006799698\n",
      "  batch 30 loss: 0.31326228380203247\n",
      "  batch 40 loss: 0.3132620841264725\n",
      "  batch 50 loss: 0.313262066245079\n",
      "  batch 60 loss: 0.3132620245218277\n",
      "  batch 70 loss: 0.3132618248462677\n",
      "  batch 80 loss: 0.31326175630092623\n",
      "LOSS train 0.31326175630092623 valid 0.4466843903064728 18.0/41\n",
      "EPOCH 20:\n",
      "  batch 10 loss: 0.3132618427276611\n",
      "  batch 20 loss: 0.31326209008693695\n",
      "  batch 30 loss: 0.313261941075325\n",
      "  batch 40 loss: 0.3132620483636856\n",
      "  batch 50 loss: 0.31326202750205995\n",
      "  batch 60 loss: 0.313261890411377\n",
      "  batch 70 loss: 0.31326213777065276\n",
      "  batch 80 loss: 0.3132620006799698\n",
      "LOSS train 0.3132620006799698 valid 0.4464443325996399 18.0/41\n",
      "EPOCH 21:\n",
      "  batch 10 loss: 0.3132619023323059\n",
      "  batch 20 loss: 0.31326192915439605\n",
      "  batch 30 loss: 0.3132621258497238\n",
      "  batch 40 loss: 0.3132619172334671\n",
      "  batch 50 loss: 0.31326212286949157\n",
      "  batch 60 loss: 0.31326200664043424\n",
      "  batch 70 loss: 0.3132618576288223\n",
      "  batch 80 loss: 0.31326182186603546\n",
      "LOSS train 0.31326182186603546 valid 0.44631245732307434 18.0/41\n",
      "EPOCH 22:\n",
      "  batch 10 loss: 0.31326188147068024\n",
      "  batch 20 loss: 0.31326195001602175\n",
      "  batch 30 loss: 0.3132621169090271\n",
      "  batch 40 loss: 0.31326197683811186\n",
      "  batch 50 loss: 0.3132618337869644\n",
      "  batch 60 loss: 0.3132619380950928\n",
      "  batch 70 loss: 0.3132618606090546\n",
      "  batch 80 loss: 0.3132619380950928\n",
      "LOSS train 0.3132619380950928 valid 0.4466899037361145 18.0/41\n",
      "EPOCH 23:\n",
      "  batch 10 loss: 0.3132620245218277\n",
      "  batch 20 loss: 0.3132619082927704\n",
      "  batch 30 loss: 0.31326192915439605\n",
      "  batch 40 loss: 0.3132621496915817\n",
      "  batch 50 loss: 0.3132618576288223\n",
      "  batch 60 loss: 0.3132618129253387\n",
      "  batch 70 loss: 0.31326188147068024\n",
      "  batch 80 loss: 0.31326201260089875\n",
      "LOSS train 0.31326201260089875 valid 0.4463621973991394 18.0/41\n",
      "EPOCH 24:\n",
      "  batch 10 loss: 0.31326210498809814\n",
      "  batch 20 loss: 0.3132618576288223\n",
      "  batch 30 loss: 0.3132619261741638\n",
      "  batch 40 loss: 0.31326177418231965\n",
      "  batch 50 loss: 0.3132618606090546\n",
      "  batch 60 loss: 0.3132619023323059\n",
      "  batch 70 loss: 0.3132619380950928\n",
      "  batch 80 loss: 0.3132619380950928\n",
      "LOSS train 0.3132619380950928 valid 0.4464602470397949 18.0/41\n",
      "EPOCH 25:\n",
      "  batch 10 loss: 0.3132620334625244\n",
      "  batch 20 loss: 0.313262003660202\n",
      "  batch 30 loss: 0.3132620334625244\n",
      "  batch 40 loss: 0.3132618010044098\n",
      "  batch 50 loss: 0.31326172649860384\n",
      "  batch 60 loss: 0.31326186954975127\n",
      "  batch 70 loss: 0.313261878490448\n",
      "  batch 80 loss: 0.3132618427276611\n",
      "LOSS train 0.3132618427276611 valid 0.4460742771625519 18.0/41\n",
      "EPOCH 26:\n",
      "  batch 10 loss: 0.31326192021369936\n",
      "  batch 20 loss: 0.3132618129253387\n",
      "  batch 30 loss: 0.3132620841264725\n",
      "  batch 40 loss: 0.31326197981834414\n",
      "  batch 50 loss: 0.3132618606090546\n",
      "  batch 60 loss: 0.3132618933916092\n",
      "  batch 70 loss: 0.31326178312301634\n",
      "  batch 80 loss: 0.31326185166835785\n",
      "LOSS train 0.31326185166835785 valid 0.4461762309074402 18.0/41\n",
      "EPOCH 27:\n",
      "  batch 10 loss: 0.31326198279857637\n",
      "  batch 20 loss: 0.3132618337869644\n",
      "  batch 30 loss: 0.31326186656951904\n",
      "  batch 40 loss: 0.3132617473602295\n",
      "  batch 50 loss: 0.3132619738578796\n",
      "  batch 60 loss: 0.313261952996254\n",
      "  batch 70 loss: 0.3132619023323059\n",
      "  batch 80 loss: 0.31326186954975127\n",
      "LOSS train 0.31326186954975127 valid 0.44632166624069214 18.0/41\n",
      "EPOCH 28:\n",
      "  batch 10 loss: 0.31326183676719666\n",
      "  batch 20 loss: 0.3132619738578796\n",
      "  batch 30 loss: 0.31326175332069395\n",
      "  batch 40 loss: 0.31326186954975127\n",
      "  batch 50 loss: 0.3132619231939316\n",
      "  batch 60 loss: 0.3132618278264999\n",
      "  batch 70 loss: 0.3132618486881256\n",
      "  batch 80 loss: 0.3132618874311447\n",
      "LOSS train 0.3132618874311447 valid 0.4461112916469574 18.0/41\n",
      "EPOCH 29:\n",
      "  batch 10 loss: 0.3132617354393005\n",
      "  batch 20 loss: 0.3132618486881256\n",
      "  batch 30 loss: 0.3132617861032486\n",
      "  batch 40 loss: 0.3132619708776474\n",
      "  batch 50 loss: 0.31326199769973756\n",
      "  batch 60 loss: 0.3132618874311447\n",
      "  batch 70 loss: 0.3132618606090546\n",
      "  batch 80 loss: 0.3132618755102158\n",
      "LOSS train 0.3132618755102158 valid 0.44629958271980286 18.0/41\n",
      "EPOCH 30:\n",
      "  batch 10 loss: 0.3132616877555847\n",
      "  batch 20 loss: 0.3132617175579071\n",
      "  batch 30 loss: 0.3132620245218277\n",
      "  batch 40 loss: 0.31326196193695066\n",
      "  batch 50 loss: 0.3132617950439453\n",
      "  batch 60 loss: 0.3132618427276611\n",
      "  batch 70 loss: 0.3132618576288223\n",
      "  batch 80 loss: 0.3132620006799698\n",
      "LOSS train 0.3132620006799698 valid 0.44613635540008545 18.0/41\n",
      "EPOCH 31:\n",
      "  batch 10 loss: 0.31326182186603546\n",
      "  batch 20 loss: 0.31326197981834414\n",
      "  batch 30 loss: 0.31326180398464204\n",
      "  batch 40 loss: 0.3132617503404617\n",
      "  batch 50 loss: 0.3132617175579071\n",
      "  batch 60 loss: 0.3132618457078934\n",
      "  batch 70 loss: 0.3132619321346283\n",
      "  batch 80 loss: 0.31326192021369936\n",
      "LOSS train 0.31326192021369936 valid 0.44620054960250854 18.0/41\n",
      "EPOCH 32:\n",
      "  batch 10 loss: 0.3132617950439453\n",
      "  batch 20 loss: 0.31326192021369936\n",
      "  batch 30 loss: 0.3132617503404617\n",
      "  batch 40 loss: 0.31326169073581694\n",
      "  batch 50 loss: 0.3132618576288223\n",
      "  batch 60 loss: 0.31326189637184143\n",
      "  batch 70 loss: 0.3132618933916092\n",
      "  batch 80 loss: 0.313261941075325\n",
      "LOSS train 0.313261941075325 valid 0.4461183249950409 20.0/41\n",
      "EPOCH 33:\n",
      "  batch 10 loss: 0.313261890411377\n",
      "  batch 20 loss: 0.3132617771625519\n",
      "  batch 30 loss: 0.31326172649860384\n",
      "  batch 40 loss: 0.3132619112730026\n",
      "  batch 50 loss: 0.31326183676719666\n",
      "  batch 60 loss: 0.31326186954975127\n",
      "  batch 70 loss: 0.3132617622613907\n",
      "  batch 80 loss: 0.31326195001602175\n",
      "LOSS train 0.31326195001602175 valid 0.4462120234966278 20.0/41\n",
      "EPOCH 34:\n",
      "  batch 10 loss: 0.31326195001602175\n",
      "  batch 20 loss: 0.31326183676719666\n",
      "  batch 30 loss: 0.31326175928115846\n",
      "  batch 40 loss: 0.31326189637184143\n",
      "  batch 50 loss: 0.3132618457078934\n",
      "  batch 60 loss: 0.31326170563697814\n",
      "  batch 70 loss: 0.31326183676719666\n",
      "  batch 80 loss: 0.31326180696487427\n",
      "LOSS train 0.31326180696487427 valid 0.44615861773490906 20.0/41\n",
      "EPOCH 35:\n",
      "  batch 10 loss: 0.31326193511486056\n",
      "  batch 20 loss: 0.31326181888580323\n",
      "  batch 30 loss: 0.3132618099451065\n",
      "  batch 40 loss: 0.3132618010044098\n",
      "  batch 50 loss: 0.3132618635892868\n",
      "  batch 60 loss: 0.313261815905571\n",
      "  batch 70 loss: 0.31326175928115846\n",
      "  batch 80 loss: 0.3132617652416229\n",
      "LOSS train 0.3132617652416229 valid 0.44602110981941223 20.0/41\n",
      "EPOCH 36:\n",
      "  batch 10 loss: 0.3132618486881256\n",
      "  batch 20 loss: 0.3132618427276611\n",
      "  batch 30 loss: 0.31326175630092623\n",
      "  batch 40 loss: 0.3132618099451065\n",
      "  batch 50 loss: 0.3132619231939316\n",
      "  batch 60 loss: 0.31326180398464204\n",
      "  batch 70 loss: 0.3132617175579071\n",
      "  batch 80 loss: 0.31326179802417753\n",
      "LOSS train 0.31326179802417753 valid 0.446200966835022 20.0/41\n",
      "EPOCH 37:\n",
      "  batch 10 loss: 0.3132618308067322\n",
      "  batch 20 loss: 0.31326169669628146\n",
      "  batch 30 loss: 0.31326175332069395\n",
      "  batch 40 loss: 0.3132617712020874\n",
      "  batch 50 loss: 0.31326175332069395\n",
      "  batch 60 loss: 0.3132618576288223\n",
      "  batch 70 loss: 0.31326175630092623\n",
      "  batch 80 loss: 0.31326195895671843\n",
      "LOSS train 0.31326195895671843 valid 0.44625386595726013 20.0/41\n",
      "EPOCH 38:\n",
      "  batch 10 loss: 0.31326185166835785\n",
      "  batch 20 loss: 0.3132617861032486\n",
      "  batch 30 loss: 0.31326172053813933\n",
      "  batch 40 loss: 0.31326185166835785\n",
      "  batch 50 loss: 0.3132617503404617\n",
      "  batch 60 loss: 0.31326175630092623\n",
      "  batch 70 loss: 0.3132618129253387\n",
      "  batch 80 loss: 0.3132618755102158\n",
      "LOSS train 0.3132618755102158 valid 0.44636666774749756 20.0/41\n",
      "EPOCH 39:\n",
      "  batch 10 loss: 0.3132617861032486\n",
      "  batch 20 loss: 0.31326172947883607\n",
      "  batch 30 loss: 0.3132617771625519\n",
      "  batch 40 loss: 0.3132618725299835\n",
      "  batch 50 loss: 0.3132618486881256\n",
      "  batch 60 loss: 0.3132617771625519\n",
      "  batch 70 loss: 0.3132617771625519\n",
      "  batch 80 loss: 0.3132618010044098\n",
      "LOSS train 0.3132618010044098 valid 0.446088969707489 20.0/41\n",
      "EPOCH 40:\n",
      "  batch 10 loss: 0.31326174139976504\n",
      "  batch 20 loss: 0.31326172351837156\n",
      "  batch 30 loss: 0.3132619380950928\n",
      "  batch 40 loss: 0.31326182186603546\n",
      "  batch 50 loss: 0.31326180398464204\n",
      "  batch 60 loss: 0.31326175332069395\n",
      "  batch 70 loss: 0.3132617354393005\n",
      "  batch 80 loss: 0.31326174437999726\n",
      "LOSS train 0.31326174437999726 valid 0.44627073407173157 20.0/41\n",
      "EPOCH 41:\n",
      "  batch 10 loss: 0.31326178908348085\n",
      "  batch 20 loss: 0.31326174139976504\n",
      "  batch 30 loss: 0.31326172053813933\n",
      "  batch 40 loss: 0.3132618308067322\n",
      "  batch 50 loss: 0.31326173841953275\n",
      "  batch 60 loss: 0.31326179802417753\n",
      "  batch 70 loss: 0.3132617503404617\n",
      "  batch 80 loss: 0.3132618635892868\n",
      "LOSS train 0.3132618635892868 valid 0.4464297592639923 20.0/41\n",
      "EPOCH 42:\n",
      "  batch 10 loss: 0.3132617861032486\n",
      "  batch 20 loss: 0.31326175630092623\n",
      "  batch 30 loss: 0.31326172351837156\n",
      "  batch 40 loss: 0.3132617652416229\n",
      "  batch 50 loss: 0.3132618099451065\n",
      "  batch 60 loss: 0.31326178312301634\n",
      "  batch 70 loss: 0.31326176822185514\n",
      "  batch 80 loss: 0.31326174437999726\n",
      "LOSS train 0.31326174437999726 valid 0.4461108148097992 20.0/41\n",
      "EPOCH 43:\n",
      "  batch 10 loss: 0.3132618308067322\n",
      "  batch 20 loss: 0.31326174437999726\n",
      "  batch 30 loss: 0.3132617473602295\n",
      "  batch 40 loss: 0.31326173841953275\n",
      "  batch 50 loss: 0.3132617473602295\n",
      "  batch 60 loss: 0.31326180398464204\n",
      "  batch 70 loss: 0.31326178312301634\n",
      "  batch 80 loss: 0.3132617622613907\n",
      "LOSS train 0.3132617622613907 valid 0.44619667530059814 20.0/41\n",
      "EPOCH 44:\n",
      "  batch 10 loss: 0.3132617861032486\n",
      "  batch 20 loss: 0.3132616698741913\n",
      "  batch 30 loss: 0.31326177418231965\n",
      "  batch 40 loss: 0.31326170861721037\n",
      "  batch 50 loss: 0.3132618308067322\n",
      "  batch 60 loss: 0.3132618635892868\n",
      "  batch 70 loss: 0.3132617175579071\n",
      "  batch 80 loss: 0.3132617771625519\n",
      "LOSS train 0.3132617771625519 valid 0.4460817277431488 20.0/41\n",
      "EPOCH 45:\n",
      "  batch 10 loss: 0.3132617652416229\n",
      "  batch 20 loss: 0.31326172649860384\n",
      "  batch 30 loss: 0.31326179802417753\n",
      "  batch 40 loss: 0.31326175332069395\n",
      "  batch 50 loss: 0.31326172947883607\n",
      "  batch 60 loss: 0.3132617354393005\n",
      "  batch 70 loss: 0.31326174139976504\n",
      "  batch 80 loss: 0.31326177418231965\n",
      "LOSS train 0.31326177418231965 valid 0.4460946023464203 20.0/41\n",
      "EPOCH 46:\n",
      "  batch 10 loss: 0.3132617801427841\n",
      "  batch 20 loss: 0.3132618010044098\n",
      "  batch 30 loss: 0.31326172947883607\n",
      "  batch 40 loss: 0.31326179802417753\n",
      "  batch 50 loss: 0.31326172649860384\n",
      "  batch 60 loss: 0.31326172947883607\n",
      "  batch 70 loss: 0.31326174437999726\n",
      "  batch 80 loss: 0.31326175630092623\n",
      "LOSS train 0.31326175630092623 valid 0.4462484121322632 20.0/41\n",
      "EPOCH 47:\n",
      "  batch 10 loss: 0.3132616877555847\n",
      "  batch 20 loss: 0.31326172351837156\n",
      "  batch 30 loss: 0.31326177418231965\n",
      "  batch 40 loss: 0.3132618099451065\n",
      "  batch 50 loss: 0.3132617652416229\n",
      "  batch 60 loss: 0.31326172053813933\n",
      "  batch 70 loss: 0.31326169073581694\n",
      "  batch 80 loss: 0.3132618397474289\n",
      "LOSS train 0.3132618397474289 valid 0.44614753127098083 20.0/41\n",
      "EPOCH 48:\n",
      "  batch 10 loss: 0.31326174139976504\n",
      "  batch 20 loss: 0.3132618606090546\n",
      "  batch 30 loss: 0.3132617473602295\n",
      "  batch 40 loss: 0.3132617026567459\n",
      "  batch 50 loss: 0.31326172351837156\n",
      "  batch 60 loss: 0.3132617324590683\n",
      "  batch 70 loss: 0.3132618129253387\n",
      "  batch 80 loss: 0.3132616877555847\n",
      "LOSS train 0.3132616877555847 valid 0.4459078013896942 20.0/41\n",
      "EPOCH 49:\n",
      "  batch 10 loss: 0.31326175630092623\n",
      "  batch 20 loss: 0.3132618129253387\n",
      "  batch 30 loss: 0.31326168179512026\n",
      "  batch 40 loss: 0.3132617026567459\n",
      "  batch 50 loss: 0.31326174139976504\n",
      "  batch 60 loss: 0.31326174437999726\n",
      "  batch 70 loss: 0.31326172947883607\n",
      "  batch 80 loss: 0.3132617652416229\n",
      "LOSS train 0.3132617652416229 valid 0.44591429829597473 20.0/41\n",
      "EPOCH 50:\n",
      "  batch 10 loss: 0.31326169371604917\n",
      "  batch 20 loss: 0.31326176822185514\n",
      "  batch 30 loss: 0.3132617861032486\n",
      "  batch 40 loss: 0.3132616847753525\n",
      "  batch 50 loss: 0.3132617175579071\n",
      "  batch 60 loss: 0.31326176822185514\n",
      "  batch 70 loss: 0.31326171159744265\n",
      "  batch 80 loss: 0.31326174139976504\n",
      "LOSS train 0.31326174139976504 valid 0.4460350275039673 20.0/41\n",
      "EPOCH 51:\n",
      "  batch 10 loss: 0.3132616877555847\n",
      "  batch 20 loss: 0.31326173841953275\n",
      "  batch 30 loss: 0.3132617920637131\n",
      "  batch 40 loss: 0.31326174139976504\n",
      "  batch 50 loss: 0.3132617324590683\n",
      "  batch 60 loss: 0.3132616728544235\n",
      "  batch 70 loss: 0.31326178312301634\n",
      "  batch 80 loss: 0.31326175332069395\n",
      "LOSS train 0.31326175332069395 valid 0.44602546095848083 20.0/41\n",
      "EPOCH 52:\n",
      "  batch 10 loss: 0.31326174139976504\n",
      "  batch 20 loss: 0.3132617145776749\n",
      "  batch 30 loss: 0.3132617175579071\n",
      "  batch 40 loss: 0.31326175928115846\n",
      "  batch 50 loss: 0.31326172947883607\n",
      "  batch 60 loss: 0.3132617026567459\n",
      "  batch 70 loss: 0.3132617354393005\n",
      "  batch 80 loss: 0.31326175630092623\n",
      "LOSS train 0.31326175630092623 valid 0.4461303949356079 20.0/41\n",
      "EPOCH 53:\n",
      "  batch 10 loss: 0.31326167583465575\n",
      "  batch 20 loss: 0.3132617145776749\n",
      "  batch 30 loss: 0.31326174437999726\n",
      "  batch 40 loss: 0.31326172351837156\n",
      "  batch 50 loss: 0.31326176822185514\n",
      "  batch 60 loss: 0.31326172947883607\n",
      "  batch 70 loss: 0.31326170563697814\n",
      "  batch 80 loss: 0.3132617503404617\n",
      "LOSS train 0.3132617503404617 valid 0.4459417164325714 20.0/41\n",
      "EPOCH 54:\n",
      "  batch 10 loss: 0.31326174139976504\n",
      "  batch 20 loss: 0.31326169073581694\n",
      "  batch 30 loss: 0.3132617712020874\n",
      "  batch 40 loss: 0.31326171159744265\n",
      "  batch 50 loss: 0.3132617712020874\n",
      "  batch 60 loss: 0.31326171159744265\n",
      "  batch 70 loss: 0.31326172351837156\n",
      "  batch 80 loss: 0.3132617026567459\n",
      "LOSS train 0.3132617026567459 valid 0.44598427414894104 20.0/41\n",
      "EPOCH 55:\n",
      "  batch 10 loss: 0.31326172947883607\n",
      "  batch 20 loss: 0.31326169669628146\n",
      "  batch 30 loss: 0.31326172947883607\n",
      "  batch 40 loss: 0.31326172947883607\n",
      "  batch 50 loss: 0.31326169371604917\n",
      "  batch 60 loss: 0.31326169669628146\n",
      "  batch 70 loss: 0.31326172351837156\n",
      "  batch 80 loss: 0.31326178312301634\n",
      "LOSS train 0.31326178312301634 valid 0.4460127651691437 20.0/41\n",
      "EPOCH 56:\n",
      "  batch 10 loss: 0.31326174139976504\n",
      "  batch 20 loss: 0.31326174139976504\n",
      "  batch 30 loss: 0.3132617175579071\n",
      "  batch 40 loss: 0.31326170861721037\n",
      "  batch 50 loss: 0.31326170563697814\n",
      "  batch 60 loss: 0.31326172649860384\n",
      "  batch 70 loss: 0.3132617324590683\n",
      "  batch 80 loss: 0.31326169371604917\n",
      "LOSS train 0.31326169371604917 valid 0.44592219591140747 20.0/41\n",
      "EPOCH 57:\n",
      "  batch 10 loss: 0.31326172649860384\n",
      "  batch 20 loss: 0.3132616639137268\n",
      "  batch 30 loss: 0.31326175630092623\n",
      "  batch 40 loss: 0.3132617503404617\n",
      "  batch 50 loss: 0.31326170861721037\n",
      "  batch 60 loss: 0.31326170563697814\n",
      "  batch 70 loss: 0.3132617145776749\n",
      "  batch 80 loss: 0.31326172053813933\n",
      "LOSS train 0.31326172053813933 valid 0.4456712603569031 20.0/41\n",
      "EPOCH 58:\n",
      "  batch 10 loss: 0.3132617175579071\n",
      "  batch 20 loss: 0.31326172947883607\n",
      "  batch 30 loss: 0.3132617354393005\n",
      "  batch 40 loss: 0.31326169371604917\n",
      "  batch 50 loss: 0.31326169073581694\n",
      "  batch 60 loss: 0.31326178908348085\n",
      "  batch 70 loss: 0.31326166689395907\n",
      "  batch 80 loss: 0.31326169073581694\n",
      "LOSS train 0.31326169073581694 valid 0.44581887125968933 20.0/41\n",
      "EPOCH 59:\n",
      "  batch 10 loss: 0.3132617145776749\n",
      "  batch 20 loss: 0.31326167583465575\n",
      "  batch 30 loss: 0.3132617503404617\n",
      "  batch 40 loss: 0.3132617145776749\n",
      "  batch 50 loss: 0.31326172947883607\n",
      "  batch 60 loss: 0.3132616877555847\n",
      "  batch 70 loss: 0.31326170861721037\n",
      "  batch 80 loss: 0.31326174139976504\n",
      "LOSS train 0.31326174139976504 valid 0.44585981965065 20.0/41\n",
      "EPOCH 60:\n",
      "  batch 10 loss: 0.31326172649860384\n",
      "  batch 20 loss: 0.31326174139976504\n",
      "  batch 30 loss: 0.31326169371604917\n",
      "  batch 40 loss: 0.31326169371604917\n",
      "  batch 50 loss: 0.3132617026567459\n",
      "  batch 60 loss: 0.3132617354393005\n",
      "  batch 70 loss: 0.3132617324590683\n",
      "  batch 80 loss: 0.31326169371604917\n",
      "LOSS train 0.31326169371604917 valid 0.44581708312034607 20.0/41\n",
      "EPOCH 61:\n",
      "  batch 10 loss: 0.3132617175579071\n",
      "  batch 20 loss: 0.31326169073581694\n",
      "  batch 30 loss: 0.31326169073581694\n",
      "  batch 40 loss: 0.31326172351837156\n",
      "  batch 50 loss: 0.3132616877555847\n",
      "  batch 60 loss: 0.31326173841953275\n",
      "  batch 70 loss: 0.3132617145776749\n",
      "  batch 80 loss: 0.3132617026567459\n",
      "LOSS train 0.3132617026567459 valid 0.4455728530883789 20.0/41\n",
      "EPOCH 62:\n",
      "  batch 10 loss: 0.31326172053813933\n",
      "  batch 20 loss: 0.3132617175579071\n",
      "  batch 30 loss: 0.31326172351837156\n",
      "  batch 40 loss: 0.3132616847753525\n",
      "  batch 50 loss: 0.31326172351837156\n",
      "  batch 60 loss: 0.3132616996765137\n",
      "  batch 70 loss: 0.31326169669628146\n",
      "  batch 80 loss: 0.31326170563697814\n",
      "LOSS train 0.31326170563697814 valid 0.44576355814933777 20.0/41\n",
      "EPOCH 63:\n",
      "  batch 10 loss: 0.3132616996765137\n",
      "  batch 20 loss: 0.31326169073581694\n",
      "  batch 30 loss: 0.3132617145776749\n",
      "  batch 40 loss: 0.31326170563697814\n",
      "  batch 50 loss: 0.31326170563697814\n",
      "  batch 60 loss: 0.31326169073581694\n",
      "  batch 70 loss: 0.3132617175579071\n",
      "  batch 80 loss: 0.31326172351837156\n",
      "LOSS train 0.31326172351837156 valid 0.4455099105834961 20.0/41\n",
      "EPOCH 64:\n",
      "  batch 10 loss: 0.31326174139976504\n",
      "  batch 20 loss: 0.3132616728544235\n",
      "  batch 30 loss: 0.31326169669628146\n",
      "  batch 40 loss: 0.31326173841953275\n",
      "  batch 50 loss: 0.31326167583465575\n",
      "  batch 60 loss: 0.3132617354393005\n",
      "  batch 70 loss: 0.31326169073581694\n",
      "  batch 80 loss: 0.31326169073581694\n",
      "LOSS train 0.31326169073581694 valid 0.4456692039966583 20.0/41\n",
      "EPOCH 65:\n",
      "  batch 10 loss: 0.31326168179512026\n",
      "  batch 20 loss: 0.31326172649860384\n",
      "  batch 30 loss: 0.3132616996765137\n",
      "  batch 40 loss: 0.31326169371604917\n",
      "  batch 50 loss: 0.31326169669628146\n",
      "  batch 60 loss: 0.31326170563697814\n",
      "  batch 70 loss: 0.31326170861721037\n",
      "  batch 80 loss: 0.3132616877555847\n",
      "LOSS train 0.3132616877555847 valid 0.4455806016921997 20.0/41\n",
      "EPOCH 66:\n",
      "  batch 10 loss: 0.31326169371604917\n",
      "  batch 20 loss: 0.31326168179512026\n",
      "  batch 30 loss: 0.3132617175579071\n",
      "  batch 40 loss: 0.31326171159744265\n",
      "  batch 50 loss: 0.31326169073581694\n",
      "  batch 60 loss: 0.31326170861721037\n",
      "  batch 70 loss: 0.3132616996765137\n",
      "  batch 80 loss: 0.3132616728544235\n",
      "LOSS train 0.3132616728544235 valid 0.4455395042896271 20.0/41\n",
      "EPOCH 67:\n",
      "  batch 10 loss: 0.31326169073581694\n",
      "  batch 20 loss: 0.3132617145776749\n",
      "  batch 30 loss: 0.31326170861721037\n",
      "  batch 40 loss: 0.31326168179512026\n",
      "  batch 50 loss: 0.31326170861721037\n",
      "  batch 60 loss: 0.31326169073581694\n",
      "  batch 70 loss: 0.31326166093349456\n",
      "  batch 80 loss: 0.3132617175579071\n",
      "LOSS train 0.3132617175579071 valid 0.44557759165763855 20.0/41\n",
      "EPOCH 68:\n",
      "  batch 10 loss: 0.31326169371604917\n",
      "  batch 20 loss: 0.31326169371604917\n",
      "  batch 30 loss: 0.31326166689395907\n",
      "  batch 40 loss: 0.3132616877555847\n",
      "  batch 50 loss: 0.31326169669628146\n",
      "  batch 60 loss: 0.3132616877555847\n",
      "  batch 70 loss: 0.3132617175579071\n",
      "  batch 80 loss: 0.3132617145776749\n",
      "LOSS train 0.3132617145776749 valid 0.4453818202018738 20.0/41\n",
      "EPOCH 69:\n",
      "  batch 10 loss: 0.31326169669628146\n",
      "  batch 20 loss: 0.31326166093349456\n",
      "  batch 30 loss: 0.3132617175579071\n",
      "  batch 40 loss: 0.3132617026567459\n",
      "  batch 50 loss: 0.31326169669628146\n",
      "  batch 60 loss: 0.31326166093349456\n",
      "  batch 70 loss: 0.31326170563697814\n",
      "  batch 80 loss: 0.31326170861721037\n",
      "LOSS train 0.31326170861721037 valid 0.44549790024757385 20.0/41\n",
      "EPOCH 70:\n",
      "  batch 10 loss: 0.3132617026567459\n",
      "  batch 20 loss: 0.31326168179512026\n",
      "  batch 30 loss: 0.3132616698741913\n",
      "  batch 40 loss: 0.31326169073581694\n",
      "  batch 50 loss: 0.3132616877555847\n",
      "  batch 60 loss: 0.3132616996765137\n",
      "  batch 70 loss: 0.3132617324590683\n",
      "  batch 80 loss: 0.31326168179512026\n",
      "LOSS train 0.31326168179512026 valid 0.44521161913871765 20.0/41\n",
      "EPOCH 71:\n",
      "  batch 10 loss: 0.3132616639137268\n",
      "  batch 20 loss: 0.31326169669628146\n",
      "  batch 30 loss: 0.31326167583465575\n",
      "  batch 40 loss: 0.31326169669628146\n",
      "  batch 50 loss: 0.31326168179512026\n",
      "  batch 60 loss: 0.31326169371604917\n",
      "  batch 70 loss: 0.31326169073581694\n",
      "  batch 80 loss: 0.31326171159744265\n",
      "LOSS train 0.31326171159744265 valid 0.4453498125076294 20.0/41\n",
      "EPOCH 72:\n",
      "  batch 10 loss: 0.3132616996765137\n",
      "  batch 20 loss: 0.31326167583465575\n",
      "  batch 30 loss: 0.3132616728544235\n",
      "  batch 40 loss: 0.3132616728544235\n",
      "  batch 50 loss: 0.3132616698741913\n",
      "  batch 60 loss: 0.3132617026567459\n",
      "  batch 70 loss: 0.31326169073581694\n",
      "  batch 80 loss: 0.3132617145776749\n",
      "LOSS train 0.3132617145776749 valid 0.4451369345188141 20.0/41\n",
      "EPOCH 73:\n",
      "  batch 10 loss: 0.3132616728544235\n",
      "  batch 20 loss: 0.3132616728544235\n",
      "  batch 30 loss: 0.3132616877555847\n",
      "  batch 40 loss: 0.3132616698741913\n",
      "  batch 50 loss: 0.3132616996765137\n",
      "  batch 60 loss: 0.3132616877555847\n",
      "  batch 70 loss: 0.31326167583465575\n",
      "  batch 80 loss: 0.31326170563697814\n",
      "LOSS train 0.31326170563697814 valid 0.4450322985649109 20.0/41\n",
      "EPOCH 74:\n",
      "  batch 10 loss: 0.3132616728544235\n",
      "  batch 20 loss: 0.3132616877555847\n",
      "  batch 30 loss: 0.31326169371604917\n",
      "  batch 40 loss: 0.3132617026567459\n",
      "  batch 50 loss: 0.31326169073581694\n",
      "  batch 60 loss: 0.31326168179512026\n",
      "  batch 70 loss: 0.31326166689395907\n",
      "  batch 80 loss: 0.313261678814888\n",
      "LOSS train 0.313261678814888 valid 0.44539928436279297 20.0/41\n",
      "EPOCH 75:\n",
      "  batch 10 loss: 0.31326166689395907\n",
      "  batch 20 loss: 0.3132616847753525\n",
      "  batch 30 loss: 0.31326169371604917\n",
      "  batch 40 loss: 0.3132616996765137\n",
      "  batch 50 loss: 0.313261678814888\n",
      "  batch 60 loss: 0.313261678814888\n",
      "  batch 70 loss: 0.313261678814888\n",
      "  batch 80 loss: 0.3132616877555847\n",
      "LOSS train 0.3132616877555847 valid 0.4453008472919464 20.0/41\n",
      "EPOCH 76:\n",
      "  batch 10 loss: 0.31326169073581694\n",
      "  batch 20 loss: 0.31326167583465575\n",
      "  batch 30 loss: 0.313261678814888\n",
      "  batch 40 loss: 0.3132616698741913\n",
      "  batch 50 loss: 0.31326168179512026\n",
      "  batch 60 loss: 0.313261678814888\n",
      "  batch 70 loss: 0.31326168179512026\n",
      "  batch 80 loss: 0.31326168179512026\n",
      "LOSS train 0.31326168179512026 valid 0.445160835981369 20.0/41\n",
      "EPOCH 77:\n",
      "  batch 10 loss: 0.31326168179512026\n",
      "  batch 20 loss: 0.31326168179512026\n",
      "  batch 30 loss: 0.3132616698741913\n",
      "  batch 40 loss: 0.31326169371604917\n",
      "  batch 50 loss: 0.3132616877555847\n",
      "  batch 60 loss: 0.3132616728544235\n",
      "  batch 70 loss: 0.3132616877555847\n",
      "  batch 80 loss: 0.313261678814888\n",
      "LOSS train 0.313261678814888 valid 0.44518959522247314 20.0/41\n",
      "EPOCH 78:\n",
      "  batch 10 loss: 0.3132616728544235\n",
      "  batch 20 loss: 0.31326169669628146\n",
      "  batch 30 loss: 0.31326168179512026\n",
      "  batch 40 loss: 0.3132616847753525\n",
      "  batch 50 loss: 0.313261678814888\n",
      "  batch 60 loss: 0.3132616639137268\n",
      "  batch 70 loss: 0.31326168179512026\n",
      "  batch 80 loss: 0.31326168179512026\n",
      "LOSS train 0.31326168179512026 valid 0.4454069435596466 20.0/41\n",
      "EPOCH 79:\n",
      "  batch 10 loss: 0.31326170861721037\n",
      "  batch 20 loss: 0.31326169073581694\n",
      "  batch 30 loss: 0.3132616639137268\n",
      "  batch 40 loss: 0.3132616877555847\n",
      "  batch 50 loss: 0.3132616847753525\n",
      "  batch 60 loss: 0.31326166093349456\n",
      "  batch 70 loss: 0.3132616847753525\n",
      "  batch 80 loss: 0.31326166093349456\n",
      "LOSS train 0.31326166093349456 valid 0.44510793685913086 20.0/41\n",
      "EPOCH 80:\n",
      "  batch 10 loss: 0.313261678814888\n",
      "  batch 20 loss: 0.3132616847753525\n",
      "  batch 30 loss: 0.31326167583465575\n",
      "  batch 40 loss: 0.3132616698741913\n",
      "  batch 50 loss: 0.3132616847753525\n",
      "  batch 60 loss: 0.3132616728544235\n",
      "  batch 70 loss: 0.31326169669628146\n",
      "  batch 80 loss: 0.3132616639137268\n",
      "LOSS train 0.3132616639137268 valid 0.4451313316822052 20.0/41\n",
      "EPOCH 81:\n",
      "  batch 10 loss: 0.3132616877555847\n",
      "  batch 20 loss: 0.31326168179512026\n",
      "  batch 30 loss: 0.313261678814888\n",
      "  batch 40 loss: 0.3132616847753525\n",
      "  batch 50 loss: 0.3132616698741913\n",
      "  batch 60 loss: 0.3132616698741913\n",
      "  batch 70 loss: 0.31326168179512026\n",
      "  batch 80 loss: 0.3132616698741913\n",
      "LOSS train 0.3132616698741913 valid 0.44511911273002625 20.0/41\n",
      "EPOCH 82:\n",
      "  batch 10 loss: 0.3132616847753525\n",
      "  batch 20 loss: 0.31326168179512026\n",
      "  batch 30 loss: 0.3132616639137268\n",
      "  batch 40 loss: 0.3132616698741913\n",
      "  batch 50 loss: 0.31326166093349456\n",
      "  batch 60 loss: 0.3132617026567459\n",
      "  batch 70 loss: 0.31326166689395907\n",
      "  batch 80 loss: 0.31326169073581694\n",
      "LOSS train 0.31326169073581694 valid 0.4449559450149536 20.0/41\n",
      "EPOCH 83:\n",
      "  batch 10 loss: 0.31326167583465575\n",
      "  batch 20 loss: 0.313261678814888\n",
      "  batch 30 loss: 0.3132616698741913\n",
      "  batch 40 loss: 0.31326169073581694\n",
      "  batch 50 loss: 0.3132616877555847\n",
      "  batch 60 loss: 0.313261678814888\n",
      "  batch 70 loss: 0.31326166689395907\n",
      "  batch 80 loss: 0.3132616639137268\n",
      "LOSS train 0.3132616639137268 valid 0.4448342025279999 20.0/41\n",
      "EPOCH 84:\n",
      "  batch 10 loss: 0.3132616877555847\n",
      "  batch 20 loss: 0.3132616847753525\n",
      "  batch 30 loss: 0.313261678814888\n",
      "  batch 40 loss: 0.31326167583465575\n",
      "  batch 50 loss: 0.31326169073581694\n",
      "  batch 60 loss: 0.31326166093349456\n",
      "  batch 70 loss: 0.31326166093349456\n",
      "  batch 80 loss: 0.3132616639137268\n",
      "LOSS train 0.3132616639137268 valid 0.44480207562446594 20.0/41\n",
      "EPOCH 85:\n",
      "  batch 10 loss: 0.31326167583465575\n",
      "  batch 20 loss: 0.31326166689395907\n",
      "  batch 30 loss: 0.3132616639137268\n",
      "  batch 40 loss: 0.31326166689395907\n",
      "  batch 50 loss: 0.31326167583465575\n",
      "  batch 60 loss: 0.3132616728544235\n",
      "  batch 70 loss: 0.31326167583465575\n",
      "  batch 80 loss: 0.31326167583465575\n",
      "LOSS train 0.31326167583465575 valid 0.44479110836982727 20.0/41\n",
      "EPOCH 86:\n",
      "  batch 10 loss: 0.3132616698741913\n",
      "  batch 20 loss: 0.31326166093349456\n",
      "  batch 30 loss: 0.3132616877555847\n",
      "  batch 40 loss: 0.3132616698741913\n",
      "  batch 50 loss: 0.3132616639137268\n",
      "  batch 60 loss: 0.31326167583465575\n",
      "  batch 70 loss: 0.3132616698741913\n",
      "  batch 80 loss: 0.31326169371604917\n",
      "LOSS train 0.31326169371604917 valid 0.4447399079799652 20.0/41\n",
      "EPOCH 87:\n",
      "  batch 10 loss: 0.3132616639137268\n",
      "  batch 20 loss: 0.3132616728544235\n",
      "  batch 30 loss: 0.31326166689395907\n",
      "  batch 40 loss: 0.3132616847753525\n",
      "  batch 50 loss: 0.3132616639137268\n",
      "  batch 60 loss: 0.3132616728544235\n",
      "  batch 70 loss: 0.31326166689395907\n",
      "  batch 80 loss: 0.31326168179512026\n",
      "LOSS train 0.31326168179512026 valid 0.4445061981678009 20.0/41\n",
      "EPOCH 88:\n",
      "  batch 10 loss: 0.31326166689395907\n",
      "  batch 20 loss: 0.3132616698741913\n",
      "  batch 30 loss: 0.31326168179512026\n",
      "  batch 40 loss: 0.3132616728544235\n",
      "  batch 50 loss: 0.3132616698741913\n",
      "  batch 60 loss: 0.3132616728544235\n",
      "  batch 70 loss: 0.31326166093349456\n",
      "  batch 80 loss: 0.3132616698741913\n",
      "LOSS train 0.3132616698741913 valid 0.4445089101791382 20.0/41\n",
      "EPOCH 89:\n",
      "  batch 10 loss: 0.3132616698741913\n",
      "  batch 20 loss: 0.31326166689395907\n",
      "  batch 30 loss: 0.3132616698741913\n",
      "  batch 40 loss: 0.3132616698741913\n",
      "  batch 50 loss: 0.3132616698741913\n",
      "  batch 60 loss: 0.31326166689395907\n",
      "  batch 70 loss: 0.3132616639137268\n",
      "  batch 80 loss: 0.313261678814888\n",
      "LOSS train 0.313261678814888 valid 0.4444679319858551 20.0/41\n",
      "EPOCH 90:\n",
      "  batch 10 loss: 0.31326166093349456\n",
      "  batch 20 loss: 0.31326166689395907\n",
      "  batch 30 loss: 0.3132616639137268\n",
      "  batch 40 loss: 0.3132616698741913\n",
      "  batch 50 loss: 0.3132616639137268\n",
      "  batch 60 loss: 0.3132616639137268\n",
      "  batch 70 loss: 0.3132616698741913\n",
      "  batch 80 loss: 0.3132616877555847\n",
      "LOSS train 0.3132616877555847 valid 0.44458886981010437 20.0/41\n",
      "EPOCH 91:\n",
      "  batch 10 loss: 0.31326166093349456\n",
      "  batch 20 loss: 0.3132616698741913\n",
      "  batch 30 loss: 0.3132616728544235\n",
      "  batch 40 loss: 0.3132616639137268\n",
      "  batch 50 loss: 0.3132616639137268\n",
      "  batch 60 loss: 0.31326166093349456\n",
      "  batch 70 loss: 0.31326167583465575\n",
      "  batch 80 loss: 0.3132616728544235\n",
      "LOSS train 0.3132616728544235 valid 0.44422072172164917 20.0/41\n",
      "EPOCH 92:\n",
      "  batch 10 loss: 0.3132616639137268\n",
      "  batch 20 loss: 0.31326166093349456\n",
      "  batch 30 loss: 0.3132616639137268\n",
      "  batch 40 loss: 0.3132616698741913\n",
      "  batch 50 loss: 0.3132616698741913\n",
      "  batch 60 loss: 0.31326167583465575\n",
      "  batch 70 loss: 0.31326165795326233\n",
      "  batch 80 loss: 0.3132616698741913\n",
      "LOSS train 0.3132616698741913 valid 0.44415798783302307 20.0/41\n",
      "EPOCH 93:\n",
      "  batch 10 loss: 0.313261678814888\n",
      "  batch 20 loss: 0.31326166093349456\n",
      "  batch 30 loss: 0.31326166689395907\n",
      "  batch 40 loss: 0.31326166689395907\n",
      "  batch 50 loss: 0.3132616639137268\n",
      "  batch 60 loss: 0.3132616639137268\n",
      "  batch 70 loss: 0.31326166689395907\n",
      "  batch 80 loss: 0.3132616639137268\n",
      "LOSS train 0.3132616639137268 valid 0.44431954622268677 20.0/41\n",
      "EPOCH 94:\n",
      "  batch 10 loss: 0.31326166689395907\n",
      "  batch 20 loss: 0.31326166093349456\n",
      "  batch 30 loss: 0.31326166093349456\n",
      "  batch 40 loss: 0.31326165795326233\n",
      "  batch 50 loss: 0.31326166093349456\n",
      "  batch 60 loss: 0.31326165795326233\n",
      "  batch 70 loss: 0.31326167583465575\n",
      "  batch 80 loss: 0.31326167583465575\n",
      "LOSS train 0.31326167583465575 valid 0.44427481293678284 20.0/41\n",
      "EPOCH 95:\n",
      "  batch 10 loss: 0.31326166689395907\n",
      "  batch 20 loss: 0.31326166093349456\n",
      "  batch 30 loss: 0.31326166689395907\n",
      "  batch 40 loss: 0.31326166093349456\n",
      "  batch 50 loss: 0.3132616698741913\n",
      "  batch 60 loss: 0.31326166689395907\n",
      "  batch 70 loss: 0.31326166093349456\n",
      "  batch 80 loss: 0.31326166093349456\n",
      "LOSS train 0.31326166093349456 valid 0.4442845582962036 20.0/41\n",
      "EPOCH 96:\n",
      "  batch 10 loss: 0.31326166689395907\n",
      "  batch 20 loss: 0.31326165795326233\n",
      "  batch 30 loss: 0.31326166689395907\n",
      "  batch 40 loss: 0.3132616639137268\n",
      "  batch 50 loss: 0.3132616639137268\n",
      "  batch 60 loss: 0.3132616698741913\n",
      "  batch 70 loss: 0.31326166093349456\n",
      "  batch 80 loss: 0.3132616639137268\n",
      "LOSS train 0.3132616639137268 valid 0.4443519413471222 20.0/41\n",
      "EPOCH 97:\n",
      "  batch 10 loss: 0.3132616728544235\n",
      "  batch 20 loss: 0.31326166689395907\n",
      "  batch 30 loss: 0.31326166093349456\n",
      "  batch 40 loss: 0.31326165795326233\n",
      "  batch 50 loss: 0.31326166093349456\n",
      "  batch 60 loss: 0.3132616639137268\n",
      "  batch 70 loss: 0.31326166689395907\n",
      "  batch 80 loss: 0.31326165795326233\n",
      "LOSS train 0.31326165795326233 valid 0.4442676603794098 20.0/41\n",
      "EPOCH 98:\n",
      "  batch 10 loss: 0.31326166689395907\n",
      "  batch 20 loss: 0.31326165795326233\n",
      "  batch 30 loss: 0.31326166689395907\n",
      "  batch 40 loss: 0.3132616639137268\n",
      "  batch 50 loss: 0.31326166093349456\n",
      "  batch 60 loss: 0.31326166093349456\n",
      "  batch 70 loss: 0.3132616639137268\n",
      "  batch 80 loss: 0.3132616639137268\n",
      "LOSS train 0.3132616639137268 valid 0.4444322884082794 20.0/41\n",
      "EPOCH 99:\n",
      "  batch 10 loss: 0.3132616639137268\n",
      "  batch 20 loss: 0.31326166093349456\n",
      "  batch 30 loss: 0.31326166093349456\n",
      "  batch 40 loss: 0.31326166093349456\n",
      "  batch 50 loss: 0.31326166093349456\n",
      "  batch 60 loss: 0.3132616698741913\n",
      "  batch 70 loss: 0.31326166093349456\n",
      "  batch 80 loss: 0.3132616639137268\n",
      "LOSS train 0.3132616639137268 valid 0.44439753890037537 20.0/41\n",
      "EPOCH 100:\n",
      "  batch 10 loss: 0.31326166093349456\n",
      "  batch 20 loss: 0.31326165795326233\n",
      "  batch 30 loss: 0.3132616698741913\n",
      "  batch 40 loss: 0.3132616698741913\n",
      "  batch 50 loss: 0.31326166093349456\n",
      "  batch 60 loss: 0.31326166093349456\n",
      "  batch 70 loss: 0.31326166093349456\n",
      "  batch 80 loss: 0.31326166093349456\n",
      "LOSS train 0.31326166093349456 valid 0.44416484236717224 20.0/41\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "loss_list = torch.zeros((EPOCHS,))\n",
    "accuracy_list = torch.zeros((EPOCHS,))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "    # Ponemos el modelo en modo entrenamento\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch)\n",
    "    loss_list[epoch] = avg_loss\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "\n",
    "        correct = (torch.argmax(voutputs, dim=0) == vlabels).type(torch.FloatTensor)\n",
    "        accuracy_list[epoch] += correct.sum()\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {} {}/{}'.format(avg_loss, avg_vloss,accuracy_list[epoch],int(lonxitudeDataset*0.2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eea787ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epochs')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gráfica\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "ax1.plot(accuracy_list)\n",
    "ax1.set_ylabel(\"validation accuracy\")\n",
    "ax2.plot(loss_list)\n",
    "ax2.set_ylabel(\"validation loss\")\n",
    "ax2.set_xlabel(\"epochs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
